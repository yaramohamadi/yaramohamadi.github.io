---
title: "Post-Regularization: A survey on generalization after overfitting"
collection: publications
permalink: /publication/2023-post-regularization
excerpt: 'Machine learning relies heavily on regularization, as it allows better generalization to unknown data, even with imperfect optimization procedures and datasets. There are, however, major problems with regularization that have received little attention so far. Regularization methods are traditionally integrated inseparably into the training process, and adjusting their hyperparameters on a trained model usually requires retraining the model from scratch. Furthermore, with deep learning models becoming more complex and hard to train, the use of pre-trained models is becoming more common. We argue that being able to regularize machine learning models more efficiently is an increasingly viable concept, especially for pretrained models. This ideally requires the disentanglement of the regularization process from model training and being able to apply it as a post-processing step with little cost. There are subtle traces of this concept which we call \textit{Post-Regularization} in a wide range of existing works from several domains. However, there exists no unified view of this concept. In this work, we provide a novel taxonomy of regularization, from the perspective of the time it is introduced with respect to the model training. We hope that this work provides a foundation for future related work around \textit{Post-Regularization}.'
date: 2023-01-01
venue: 'yaramohamadi.github.io (In preperation, Target: JMLR)'
paperurl: ''
citation: 'Bahram, YM. (2023). &quot;Post-Regularization: A survey on generalization after overfitting&quot; <i>yaramohamadi.github.io</i>'
---

Machine learning relies heavily on regularization, as it allows better generalization to unknown data, even with imperfect optimization procedures and datasets. There are, however, major problems with regularization that have received little attention so far. Regularization methods are traditionally integrated inseparably into the training process, and adjusting their hyperparameters on a trained model usually requires retraining the model from scratch. Furthermore, with deep learning models becoming more complex and hard to train, the use of pre-trained models is becoming more common. We argue that being able to regularize machine learning models more efficiently is an increasingly viable concept, especially for pretrained models. This ideally requires the disentanglement of the regularization process from model training and being able to apply it as a post-processing step with little cost. There are subtle traces of this concept which we call \textit{Post-Regularization} in a wide range of existing works from several domains. However, there exists no unified view of this concept. In this work, we provide a novel taxonomy of regularization, from the perspective of the time it is introduced with respect to the model training. We hope that this work provides a foundation for future related work around \textit{Post-Regularization}.

[Download Draft Manuscript here (TBA)]()

Recommended citation: Bahram, YM. (2023). "Post-Regularization: A survey on generalization after overfitting" <i>yaramohamadi.github.io</i>

